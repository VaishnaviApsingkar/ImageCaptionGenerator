{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:24.989646Z","iopub.execute_input":"2024-12-04T14:19:24.990181Z","iopub.status.idle":"2024-12-04T14:19:24.995989Z","shell.execute_reply.started":"2024-12-04T14:19:24.990143Z","shell.execute_reply":"2024-12-04T14:19:24.994928Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:24.997307Z","iopub.execute_input":"2024-12-04T14:19:24.997651Z","iopub.status.idle":"2024-12-04T14:19:25.010946Z","shell.execute_reply.started":"2024-12-04T14:19:24.997624Z","shell.execute_reply":"2024-12-04T14:19:25.010224Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# VGG16 model\nimage_model = VGG16()\nimage_model = Model(inputs=image_model.inputs, outputs=image_model.layers[-2].output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:25.012520Z","iopub.execute_input":"2024-12-04T14:19:25.012856Z","iopub.status.idle":"2024-12-04T14:19:27.317660Z","shell.execute_reply.started":"2024-12-04T14:19:25.012819Z","shell.execute_reply":"2024-12-04T14:19:27.316680Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Extract image features\nimage_features = {}\nimage_directory = os.path.join(BASE_DIR, 'Images')\n\nfor image_name in tqdm(os.listdir(image_directory)):\n   # Load and preprocess the image\n   image_path = os.path.join(image_directory, image_name)\n   image = load_img(image_path, target_size=(224, 224))\n   image = img_to_array(image)\n   image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n   image = preprocess_input(image)\n\n   # Extract features using the image model\n   image_feature = image_model.predict(image, verbose=0)\n\n   # Store the feature with the image ID\n   image_id = image_name.split('.')[0]\n   image_features[image_id] = image_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:19:27.319344Z","iopub.execute_input":"2024-12-04T14:19:27.319610Z","iopub.status.idle":"2024-12-04T14:27:41.029677Z","shell.execute_reply.started":"2024-12-04T14:19:27.319583Z","shell.execute_reply":"2024-12-04T14:27:41.028817Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8091 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b391bff9f9ed42629701257c733848cd"}},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# Store image features in a pickle file\npickle.dump(image_features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.030988Z","iopub.execute_input":"2024-12-04T14:27:41.031696Z","iopub.status.idle":"2024-12-04T14:27:41.436615Z","shell.execute_reply.started":"2024-12-04T14:27:41.031650Z","shell.execute_reply":"2024-12-04T14:27:41.435756Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.439727Z","iopub.execute_input":"2024-12-04T14:27:41.440194Z","iopub.status.idle":"2024-12-04T14:27:41.596881Z","shell.execute_reply.started":"2024-12-04T14:27:41.440138Z","shell.execute_reply":"2024-12-04T14:27:41.596121Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.598544Z","iopub.execute_input":"2024-12-04T14:27:41.598787Z","iopub.status.idle":"2024-12-04T14:27:41.605548Z","shell.execute_reply.started":"2024-12-04T14:27:41.598762Z","shell.execute_reply":"2024-12-04T14:27:41.604824Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Create mapping of image IDs to captions\nimage_captions = {}\nfor line in tqdm(captions_doc.split('\\n')):\n   if len(line) < 2:\n       continue\n   tokens = line.split(',')\n   image_id = tokens[0].split('.')[0]\n   caption = \" \".join(tokens[1:]) #\n   if image_id not in image_captions:\n       image_captions[image_id] = []\n   image_captions[image_id].append(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.606497Z","iopub.execute_input":"2024-12-04T14:27:41.606730Z","iopub.status.idle":"2024-12-04T14:27:41.617771Z","shell.execute_reply.started":"2024-12-04T14:27:41.606707Z","shell.execute_reply":"2024-12-04T14:27:41.615156Z"}},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[45], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    caption = \" \".join(caption) # newly added\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"],"ename":"IndentationError","evalue":"unexpected indent (2377754110.py, line 9)","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"print(f\"Total number of images in dataset: {len(image_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.618344Z","iopub.status.idle":"2024-12-04T14:27:41.618656Z","shell.execute_reply.started":"2024-12-04T14:27:41.618508Z","shell.execute_reply":"2024-12-04T14:27:41.618524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_captions(image_captions):\n   for captions in image_captions.values():\n       for i, caption in enumerate(captions):\n           caption = caption.lower()\n           caption = caption.replace('[^A-Za-z]', '')\n           caption = caption.replace('\\s+', ' ')\n           caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n           captions[i] = caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.620164Z","iopub.status.idle":"2024-12-04T14:27:41.620551Z","shell.execute_reply.started":"2024-12-04T14:27:41.620332Z","shell.execute_reply":"2024-12-04T14:27:41.620355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#caption before cleaning\nimage_captions['1000268201_693b08cb0e']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.622134Z","iopub.status.idle":"2024-12-04T14:27:41.622470Z","shell.execute_reply.started":"2024-12-04T14:27:41.622276Z","shell.execute_reply":"2024-12-04T14:27:41.622290Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_captions(image_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.623873Z","iopub.status.idle":"2024-12-04T14:27:41.624237Z","shell.execute_reply.started":"2024-12-04T14:27:41.624054Z","shell.execute_reply":"2024-12-04T14:27:41.624074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_captions['1000268201_693b08cb0e']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.626138Z","iopub.status.idle":"2024-12-04T14:27:41.626454Z","shell.execute_reply.started":"2024-12-04T14:27:41.626302Z","shell.execute_reply":"2024-12-04T14:27:41.626318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_captions = []\nfor captions in image_captions.values():\n   all_captions.extend(captions)#","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.627785Z","iopub.status.idle":"2024-12-04T14:27:41.628247Z","shell.execute_reply.started":"2024-12-04T14:27:41.628016Z","shell.execute_reply":"2024-12-04T14:27:41.628041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total number of preprocessed captions: {len(all_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.630158Z","iopub.status.idle":"2024-12-04T14:27:41.630859Z","shell.execute_reply.started":"2024-12-04T14:27:41.630595Z","shell.execute_reply":"2024-12-04T14:27:41.630621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.631845Z","iopub.status.idle":"2024-12-04T14:27:41.632290Z","shell.execute_reply.started":"2024-12-04T14:27:41.632065Z","shell.execute_reply":"2024-12-04T14:27:41.632088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#prints the length of longest caption\nmax_length = max(len(caption.split()) for caption in all_captions)\nmax_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.633792Z","iopub.status.idle":"2024-12-04T14:27:41.634239Z","shell.execute_reply.started":"2024-12-04T14:27:41.634014Z","shell.execute_reply":"2024-12-04T14:27:41.634037Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split into test and train dataset\nimage_ids = list(image_captions.keys())\nsplit = int(len(image_ids) * 0.9)\ntrain_ids = image_ids[:split]\ntest_ids = image_ids[split:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.635289Z","iopub.status.idle":"2024-12-04T14:27:41.635714Z","shell.execute_reply.started":"2024-12-04T14:27:41.635481Z","shell.execute_reply":"2024-12-04T14:27:41.635504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_generator(data_keys, image_captions, features, tokenizer, max_length, vocab_size, batch_size):\n    X1, X2, y = [], [], []\n    \n    while True:\n        for key in data_keys:\n            for caption in image_captions[key]:\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # Right-pad the sequence\n                    in_seq = tf.keras.preprocessing.sequence.pad_sequences(\n                        [in_seq], \n                        maxlen=max_length,\n                        padding='post',\n                        truncating='post'\n                    )[0]\n                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n                    \n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n                    \n                    if len(X1) == batch_size:\n                        X1_array = tf.convert_to_tensor(np.array(X1), dtype=tf.float32)\n                        X2_array = tf.convert_to_tensor(np.array(X2), dtype=tf.float32)\n                        y_array = tf.convert_to_tensor(np.array(y), dtype=tf.float32)\n                        \n                        yield (X1_array, X2_array), y_array\n                        \n                        X1, X2, y = [], [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.637605Z","iopub.status.idle":"2024-12-04T14:27:41.637919Z","shell.execute_reply.started":"2024-12-04T14:27:41.637775Z","shell.execute_reply":"2024-12-04T14:27:41.637791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image encoding model\nimage_input = Input(shape=(4096,))\nencoded_image = Dropout(0.4)(image_input)\nencoded_image = Dense(256, activation='relu')(encoded_image)\n\n# Caption encoding model \ncaption_input = Input(shape=(max_length,))\ncaption_embedding = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\ncaption_dropout = Dropout(0.4)(caption_embedding)\nencoded_caption = LSTM(256)(caption_dropout)\n\n# Decoder model\nmerged_features = add([encoded_image, encoded_caption])\ndecoder_dense = Dense(256, activation='relu')(merged_features)\noutput_layer = Dense(vocab_size, activation='softmax')(decoder_dense)\n\n# Compile the model\ncaption_generator = Model(inputs=[image_input, caption_input], outputs=output_layer)\ncaption_generator.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.638783Z","iopub.status.idle":"2024-12-04T14:27:41.639089Z","shell.execute_reply.started":"2024-12-04T14:27:41.638915Z","shell.execute_reply":"2024-12-04T14:27:41.638930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 25\nbatch_size = 64\nsteps = len(train_ids) // batch_size\n\nfeature_shape = next(iter(image_features.values()))[0].shape\n\n# Configure the dataset\ndataset = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_ids, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size),\n    output_signature=(\n        (\n            tf.TensorSpec(shape=(batch_size, *feature_shape), dtype=tf.float32),\n            tf.TensorSpec(shape=(batch_size, max_length), dtype=tf.float32)\n        ),\n        tf.TensorSpec(shape=(batch_size, vocab_size), dtype=tf.float32)\n    )\n)\n\nfor layer in caption_generator.layers:\n    if isinstance(layer, tf.keras.layers.LSTM):\n        layer.use_cudnn = False\n\ncaption_generator.fit(dataset, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.640504Z","iopub.status.idle":"2024-12-04T14:27:41.640773Z","shell.execute_reply.started":"2024-12-04T14:27:41.640638Z","shell.execute_reply":"2024-12-04T14:27:41.640652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"caption_generator.save(os.path.join(WORKING_DIR, 'best_model.h5'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.641690Z","iopub.status.idle":"2024-12-04T14:27:41.642030Z","shell.execute_reply.started":"2024-12-04T14:27:41.641838Z","shell.execute_reply":"2024-12-04T14:27:41.641870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def idx_to_word(index, tokenizer):\n   return next((word for word, i in tokenizer.word_index.items() if i == index), None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.643228Z","iopub.status.idle":"2024-12-04T14:27:41.643522Z","shell.execute_reply.started":"2024-12-04T14:27:41.643376Z","shell.execute_reply":"2024-12-04T14:27:41.643391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length):\n   caption = 'startseq'\n   for _ in range(max_length):\n       sequence = tokenizer.texts_to_sequences([caption])[0]\n       sequence = pad_sequences([sequence], max_length)\n       yhat = model.predict([image, sequence], verbose=0)\n       yhat = np.argmax(yhat)\n       word = idx_to_word(yhat, tokenizer)\n       if word is None or word == 'endseq':\n           break\n       caption += f\" {word}\"\n   return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.645292Z","iopub.status.idle":"2024-12-04T14:27:41.645564Z","shell.execute_reply.started":"2024-12-04T14:27:41.645430Z","shell.execute_reply":"2024-12-04T14:27:41.645445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\nactual, predicted = [], []\nfor image_id in tqdm(test_ids):\n   captions = image_captions[image_id]\n   caption_prediction = predict_caption(caption_generator, image_features[image_id], tokenizer, max_length)\n   actual.append([caption.split() for caption in captions])\n   predicted.append(caption_prediction.split())\n\nprint(f\"BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)):.4f}\")\nprint(f\"BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.646390Z","iopub.status.idle":"2024-12-04T14:27:41.646667Z","shell.execute_reply.started":"2024-12-04T14:27:41.646528Z","shell.execute_reply":"2024-12-04T14:27:41.646543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ndef generate_caption(image_name):\n   image_id = image_name.split('.')[0]\n   image_path = os.path.join(BASE_DIR, 'Images', image_name)\n   image = Image.open(image_path)\n   \n   print('---Actual-Caption---')\n   for caption in image_captions[image_id]:\n       print(caption)\n   \n   predicted_caption = predict_caption(caption_generator, image_features[image_id], tokenizer, max_length)\n   print('---Predicted-Caption---')\n   print(predicted_caption)\n   \n   plt.imshow(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.647720Z","iopub.status.idle":"2024-12-04T14:27:41.648017Z","shell.execute_reply.started":"2024-12-04T14:27:41.647856Z","shell.execute_reply":"2024-12-04T14:27:41.647869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_caption(\"1009434119_febe49276a.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T17:55:50.741335Z","iopub.execute_input":"2024-12-04T17:55:50.741946Z","iopub.status.idle":"2024-12-04T17:55:50.975082Z","shell.execute_reply.started":"2024-12-04T17:55:50.741891Z","shell.execute_reply":"2024-12-04T17:55:50.973958Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgenerate_caption\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1009434119_febe49276a.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'generate_caption' is not defined"],"ename":"NameError","evalue":"name 'generate_caption' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"generate_caption(\"111497985_38e9f88856.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T14:27:41.649927Z","iopub.status.idle":"2024-12-04T14:27:41.650236Z","shell.execute_reply.started":"2024-12-04T14:27:41.650102Z","shell.execute_reply":"2024-12-04T14:27:41.650117Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}