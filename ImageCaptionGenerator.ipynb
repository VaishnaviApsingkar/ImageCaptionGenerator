{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:15:15.414797Z","iopub.execute_input":"2024-12-03T06:15:15.415366Z","iopub.status.idle":"2024-12-03T06:15:28.091900Z","shell.execute_reply.started":"2024-12-03T06:15:15.415321Z","shell.execute_reply":"2024-12-03T06:15:28.091138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/flickr8k'\nWORKING_DIR = '/kaggle/working'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:15:43.015180Z","iopub.execute_input":"2024-12-03T06:15:43.015722Z","iopub.status.idle":"2024-12-03T06:15:43.019483Z","shell.execute_reply.started":"2024-12-03T06:15:43.015694Z","shell.execute_reply":"2024-12-03T06:15:43.018596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# VGG16 model\nimage_model = VGG16()\nimage_model = Model(inputs=image_model.inputs, outputs=image_model.layers[-2].output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:15:48.694792Z","iopub.execute_input":"2024-12-03T06:15:48.695487Z","iopub.status.idle":"2024-12-03T06:16:05.732374Z","shell.execute_reply.started":"2024-12-03T06:15:48.695456Z","shell.execute_reply":"2024-12-03T06:16:05.731702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract image features\nimage_features = {}\nimage_directory = os.path.join(BASE_DIR, 'Images')\n\nfor image_name in tqdm(os.listdir(image_directory)):\n   # Load and preprocess the image\n   image_path = os.path.join(image_directory, image_name)\n   image = load_img(image_path, target_size=(224, 224))\n   image = img_to_array(image)\n   image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n   image = preprocess_input(image)\n\n   # Extract features using the image model\n   image_feature = image_model.predict(image, verbose=0)\n\n   # Store the feature with the image ID\n   image_id = image_name.split('.')[0]\n   image_features[image_id] = image_feature","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:26:07.885750Z","iopub.execute_input":"2024-12-03T06:26:07.886593Z","iopub.status.idle":"2024-12-03T06:35:00.408366Z","shell.execute_reply.started":"2024-12-03T06:26:07.886557Z","shell.execute_reply":"2024-12-03T06:35:00.407432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store image features in a pickle file\npickle.dump(image_features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:15.495114Z","iopub.execute_input":"2024-12-03T06:35:15.495555Z","iopub.status.idle":"2024-12-03T06:35:15.761792Z","shell.execute_reply.started":"2024-12-03T06:35:15.495522Z","shell.execute_reply":"2024-12-03T06:35:15.760795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load features from pickle\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:18.915223Z","iopub.execute_input":"2024-12-03T06:35:18.915780Z","iopub.status.idle":"2024-12-03T06:35:19.067039Z","shell.execute_reply.started":"2024-12-03T06:35:18.915748Z","shell.execute_reply":"2024-12-03T06:35:19.066114Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:22.110998Z","iopub.execute_input":"2024-12-03T06:35:22.111835Z","iopub.status.idle":"2024-12-03T06:35:22.145925Z","shell.execute_reply.started":"2024-12-03T06:35:22.111791Z","shell.execute_reply":"2024-12-03T06:35:22.144960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create mapping of image IDs to captions\nimage_captions = {}\nfor line in tqdm(captions_doc.split('\\n')):\n   if len(line) < 2:\n       continue\n   tokens = line.split(',')\n   image_id = tokens[0].split('.')[0]\n   caption = \" \".join(tokens[1:])\n   if image_id not in image_captions:\n       image_captions[image_id] = []\n   image_captions[image_id].append(caption)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:25.115205Z","iopub.execute_input":"2024-12-03T06:35:25.116146Z","iopub.status.idle":"2024-12-03T06:35:25.193954Z","shell.execute_reply.started":"2024-12-03T06:35:25.116109Z","shell.execute_reply":"2024-12-03T06:35:25.193043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total number of images in dataset: {len(image_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:50.795105Z","iopub.execute_input":"2024-12-03T06:35:50.795768Z","iopub.status.idle":"2024-12-03T06:35:50.800197Z","shell.execute_reply.started":"2024-12-03T06:35:50.795736Z","shell.execute_reply":"2024-12-03T06:35:50.799209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_captions(image_captions):\n   for captions in image_captions.values():\n       for i, caption in enumerate(captions):\n           caption = caption.lower()\n           caption = caption.replace('[^A-Za-z]', '')\n           caption = caption.replace('\\s+', ' ')\n           caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word) > 1]) + ' endseq'\n           captions[i] = caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:35:54.887202Z","iopub.execute_input":"2024-12-03T06:35:54.887743Z","iopub.status.idle":"2024-12-03T06:35:54.894478Z","shell.execute_reply.started":"2024-12-03T06:35:54.887690Z","shell.execute_reply":"2024-12-03T06:35:54.893487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#caption before cleaning\nimage_captions['1000268201_693b08cb0e']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:00.754577Z","iopub.execute_input":"2024-12-03T06:37:00.754895Z","iopub.status.idle":"2024-12-03T06:37:00.760425Z","shell.execute_reply.started":"2024-12-03T06:37:00.754870Z","shell.execute_reply":"2024-12-03T06:37:00.759601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_captions(image_captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:06.274767Z","iopub.execute_input":"2024-12-03T06:37:06.275098Z","iopub.status.idle":"2024-12-03T06:37:06.373041Z","shell.execute_reply.started":"2024-12-03T06:37:06.275069Z","shell.execute_reply":"2024-12-03T06:37:06.372341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_captions['1000268201_693b08cb0e']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:10.324450Z","iopub.execute_input":"2024-12-03T06:37:10.325372Z","iopub.status.idle":"2024-12-03T06:37:10.330271Z","shell.execute_reply.started":"2024-12-03T06:37:10.325320Z","shell.execute_reply":"2024-12-03T06:37:10.329493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"all_captions = []\nfor captions in image_captions.values():\n   all_captions.extend(captions)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:16.835405Z","iopub.execute_input":"2024-12-03T06:37:16.835737Z","iopub.status.idle":"2024-12-03T06:37:16.842273Z","shell.execute_reply.started":"2024-12-03T06:37:16.835711Z","shell.execute_reply":"2024-12-03T06:37:16.841395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Total number of preprocessed captions: {len(all_captions)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:21.864831Z","iopub.execute_input":"2024-12-03T06:37:21.865522Z","iopub.status.idle":"2024-12-03T06:37:21.869368Z","shell.execute_reply.started":"2024-12-03T06:37:21.865492Z","shell.execute_reply":"2024-12-03T06:37:21.868549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:37:25.535270Z","iopub.execute_input":"2024-12-03T06:37:25.535655Z","iopub.status.idle":"2024-12-03T06:37:26.050103Z","shell.execute_reply.started":"2024-12-03T06:37:25.535623Z","shell.execute_reply":"2024-12-03T06:37:26.049157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#prints the length of longest caption\nmax_length = max(len(caption.split()) for caption in all_captions)\nmax_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:38:54.035979Z","iopub.execute_input":"2024-12-03T06:38:54.036331Z","iopub.status.idle":"2024-12-03T06:38:54.068907Z","shell.execute_reply.started":"2024-12-03T06:38:54.036296Z","shell.execute_reply":"2024-12-03T06:38:54.068039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#split into test and train dataset\nimage_ids = list(image_captions.keys())\nsplit = int(len(image_ids) * 0.9)\ntrain_ids = image_ids[:split]\ntest_ids = image_ids[split:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:38:57.275398Z","iopub.execute_input":"2024-12-03T06:38:57.275786Z","iopub.status.idle":"2024-12-03T06:38:57.281041Z","shell.execute_reply.started":"2024-12-03T06:38:57.275760Z","shell.execute_reply":"2024-12-03T06:38:57.280100Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_generator(data_keys, image_captions, features, tokenizer, max_length, vocab_size, batch_size):\n    X1, X2, y = [], [], []\n    \n    while True:\n        for key in data_keys:\n            for caption in image_captions[key]:\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # Right-pad the sequence\n                    in_seq = tf.keras.preprocessing.sequence.pad_sequences(\n                        [in_seq], \n                        maxlen=max_length,\n                        padding='post',\n                        truncating='post'\n                    )[0]\n                    out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n                    \n                    X1.append(features[key][0])\n                    X2.append(in_seq)\n                    y.append(out_seq)\n                    \n                    if len(X1) == batch_size:\n                        X1_array = tf.convert_to_tensor(np.array(X1), dtype=tf.float32)\n                        X2_array = tf.convert_to_tensor(np.array(X2), dtype=tf.float32)\n                        y_array = tf.convert_to_tensor(np.array(y), dtype=tf.float32)\n                        \n                        yield (X1_array, X2_array), y_array\n                        \n                        X1, X2, y = [], [], []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:50:02.880932Z","iopub.execute_input":"2024-12-03T06:50:02.881764Z","iopub.status.idle":"2024-12-03T06:50:02.891962Z","shell.execute_reply.started":"2024-12-03T06:50:02.881718Z","shell.execute_reply":"2024-12-03T06:50:02.891061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image encoding model\nimage_input = Input(shape=(4096,))\nencoded_image = Dropout(0.4)(image_input)\nencoded_image = Dense(256, activation='relu')(encoded_image)\n\n# Caption encoding model \ncaption_input = Input(shape=(max_length,))\ncaption_embedding = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\ncaption_dropout = Dropout(0.4)(caption_embedding)\nencoded_caption = LSTM(256)(caption_dropout)\n\n# Decoder model\nmerged_features = add([encoded_image, encoded_caption])\ndecoder_dense = Dense(256, activation='relu')(merged_features)\noutput_layer = Dense(vocab_size, activation='softmax')(decoder_dense)\n\n# Compile the model\ncaption_generator = Model(inputs=[image_input, caption_input], outputs=output_layer)\ncaption_generator.compile(loss='categorical_crossentropy', optimizer='adam')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:50:06.115590Z","iopub.execute_input":"2024-12-03T06:50:06.115940Z","iopub.status.idle":"2024-12-03T06:50:06.194027Z","shell.execute_reply.started":"2024-12-03T06:50:06.115909Z","shell.execute_reply":"2024-12-03T06:50:06.193076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 25\nbatch_size = 64\nsteps = len(train_ids) // batch_size\n\nfeature_shape = next(iter(image_features.values()))[0].shape\n\n# Configure the dataset\ndataset = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_ids, image_captions, image_features, tokenizer, max_length, vocab_size, batch_size),\n    output_signature=(\n        (\n            tf.TensorSpec(shape=(batch_size, *feature_shape), dtype=tf.float32),\n            tf.TensorSpec(shape=(batch_size, max_length), dtype=tf.float32)\n        ),\n        tf.TensorSpec(shape=(batch_size, vocab_size), dtype=tf.float32)\n    )\n)\n\nfor layer in caption_generator.layers:\n    if isinstance(layer, tf.keras.layers.LSTM):\n        layer.use_cudnn = False\n\ncaption_generator.fit(dataset, epochs=epochs, steps_per_epoch=steps, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:50:17.014895Z","iopub.execute_input":"2024-12-03T06:50:17.015509Z","iopub.status.idle":"2024-12-03T06:52:27.461834Z","shell.execute_reply.started":"2024-12-03T06:50:17.015475Z","shell.execute_reply":"2024-12-03T06:52:27.461063Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"caption_generator.save(os.path.join(WORKING_DIR, 'best_model.h5'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:52:30.534786Z","iopub.execute_input":"2024-12-03T06:52:30.535117Z","iopub.status.idle":"2024-12-03T06:52:30.656859Z","shell.execute_reply.started":"2024-12-03T06:52:30.535088Z","shell.execute_reply":"2024-12-03T06:52:30.656164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def idx_to_word(index, tokenizer):\n   return next((word for word, i in tokenizer.word_index.items() if i == index), None)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:52:34.075991Z","iopub.execute_input":"2024-12-03T06:52:34.076322Z","iopub.status.idle":"2024-12-03T06:52:34.080847Z","shell.execute_reply.started":"2024-12-03T06:52:34.076292Z","shell.execute_reply":"2024-12-03T06:52:34.080003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length):\n   caption = 'startseq'\n   for _ in range(max_length):\n       sequence = tokenizer.texts_to_sequences([caption])[0]\n       sequence = pad_sequences([sequence], max_length)\n       yhat = model.predict([image, sequence], verbose=0)\n       yhat = np.argmax(yhat)\n       word = idx_to_word(yhat, tokenizer)\n       if word is None or word == 'endseq':\n           break\n       caption += f\" {word}\"\n   return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:52:40.374988Z","iopub.execute_input":"2024-12-03T06:52:40.375819Z","iopub.status.idle":"2024-12-03T06:52:40.381099Z","shell.execute_reply.started":"2024-12-03T06:52:40.375787Z","shell.execute_reply":"2024-12-03T06:52:40.380111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\nactual, predicted = [], []\nfor image_id in tqdm(test_ids):\n   captions = image_captions[image_id]\n   caption_prediction = predict_caption(caption_generator, image_features[image_id], tokenizer, max_length)\n   actual.append([caption.split() for caption in captions])\n   predicted.append(caption_prediction.split())\n\nprint(f\"BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)):.4f}\")\nprint(f\"BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T06:53:09.935909Z","iopub.execute_input":"2024-12-03T06:53:09.936722Z","iopub.status.idle":"2024-12-03T07:01:41.591107Z","shell.execute_reply.started":"2024-12-03T06:53:09.936690Z","shell.execute_reply":"2024-12-03T07:01:41.590159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\ndef generate_caption(image_name):\n   image_id = image_name.split('.')[0]\n   image_path = os.path.join(BASE_DIR, 'Images', image_name)\n   image = Image.open(image_path)\n   \n   print('---Actual-Caption---')\n   for caption in image_captions[image_id]:\n       print(caption)\n   \n   predicted_caption = predict_caption(caption_generator, image_features[image_id], tokenizer, max_length)\n   print('---Predicted-Caption---')\n   print(predicted_caption)\n   \n   plt.imshow(image)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:04:57.136049Z","iopub.execute_input":"2024-12-03T07:04:57.136874Z","iopub.status.idle":"2024-12-03T07:04:57.142165Z","shell.execute_reply.started":"2024-12-03T07:04:57.136841Z","shell.execute_reply":"2024-12-03T07:04:57.141177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_caption(\"1009434119_febe49276a.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:05:33.975757Z","iopub.execute_input":"2024-12-03T07:05:33.976114Z","iopub.status.idle":"2024-12-03T07:05:34.764016Z","shell.execute_reply.started":"2024-12-03T07:05:33.976084Z","shell.execute_reply":"2024-12-03T07:05:34.763160Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_caption(\"111497985_38e9f88856.jpg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T07:08:46.635954Z","iopub.execute_input":"2024-12-03T07:08:46.636289Z","iopub.status.idle":"2024-12-03T07:08:47.392215Z","shell.execute_reply.started":"2024-12-03T07:08:46.636259Z","shell.execute_reply":"2024-12-03T07:08:47.391422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
